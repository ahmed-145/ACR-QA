#!/usr/bin/env python3
"""
ACR-QA PR Summary Generator
Generates human-readable summaries for pull requests
"""

import sys
from pathlib import Path
from typing import List, Dict, Any
from collections import Counter

sys.path.insert(0, str(Path(__file__).parent.parent))

from DATABASE.database import Database


def generate_pr_summary(run_id: int = None) -> str:
    """
    Generate a markdown summary for a PR based on analysis findings
    
    Args:
        run_id: Analysis run ID (uses latest if not provided)
        
    Returns:
        Markdown formatted summary
    """
    db = Database()
    
    # Get the run
    if run_id:
        runs = [r for r in db.get_recent_runs(limit=100) if r['id'] == run_id]
        if not runs:
            return f"âŒ Run {run_id} not found"
        run = runs[0]
    else:
        runs = db.get_recent_runs(limit=1)
        if not runs:
            return "âŒ No analysis runs found"
        run = runs[0]
    
    # Get findings for this run
    findings = db.get_findings(run['id'])
    
    # Calculate statistics
    total = len(findings)
    severity_counts = Counter(f.get('severity', 'low') for f in findings)
    category_counts = Counter(f.get('category', 'unknown') for f in findings)
    file_counts = Counter(f.get('file_path', 'unknown') for f in findings)
    
    # Get top categories and files
    top_categories = category_counts.most_common(3)
    top_files = file_counts.most_common(3)
    
    # Critical findings
    critical_findings = [f for f in findings if f.get('severity') in ('high', 'critical')]
    
    # Build summary
    summary = f"""## ðŸ“Š ACR-QA Analysis Summary

**Repository:** {run.get('repo_name', 'Unknown')}  
**Run ID:** {run['id']}  
**Status:** {run.get('status', 'unknown')}

---

### ðŸ“ˆ Overview

| Metric | Value |
|--------|-------|
| **Total Issues** | {total} |
| **Critical/High** | {severity_counts.get('high', 0) + severity_counts.get('critical', 0)} |
| **Medium** | {severity_counts.get('medium', 0)} |
| **Low/Info** | {severity_counts.get('low', 0) + severity_counts.get('info', 0)} |

---

### ðŸ·ï¸ Top Categories

"""
    
    for cat, count in top_categories:
        summary += f"- **{cat}**: {count} issues\n"
    
    summary += """
---

### ðŸ“ Most Affected Files

"""
    
    for filepath, count in top_files:
        filename = Path(filepath).name if filepath else 'unknown'
        summary += f"- `{filename}`: {count} issues\n"
    
    if critical_findings:
        summary += f"""
---

### ðŸš¨ Critical Issues ({len(critical_findings)})

"""
        for f in critical_findings[:5]:  # Show top 5 critical
            summary += f"- **{f.get('canonical_rule_id', 'UNKNOWN')}**: {f.get('message', 'No message')[:80]}\n"
        
        if len(critical_findings) > 5:
            summary += f"\n*...and {len(critical_findings) - 5} more critical issues*\n"
    
    summary += f"""
---

### âœ… Recommendations

1. Address **{len(critical_findings)} critical issues** first
2. Review files with most issues: {', '.join(f'`{Path(f).name}`' for f, _ in top_files[:2])}
3. Consider auto-fix for low-severity items

---

*Generated by ACR-QA v2.0 | [View Full Report](dashboard)*
"""
    
    return summary


def print_summary(run_id: int = None):
    """Print summary to console"""
    print(generate_pr_summary(run_id))


if __name__ == "__main__":
    run_id = int(sys.argv[1]) if len(sys.argv) > 1 else None
    print_summary(run_id)
